# Parallel Pacman Simulator: High-Performance Multi-Agent Environment

Contributers: Po-Kai Huang, Henry Tsai, Shih-Wen Huang, Chih-Ying Liu, Max Lin

This project implements a high-performance, parallelized Pacman simulator using C(Kernel), OpenMP, and MPI, wrapped in Python. It demonstrates linear scaling(31.06x) on 32-core nodes and scales effectively to 144 distributed MPI ranks(~290k steps/s)

## ðŸ“‹ Compliance Matrix (For Graders)

We have met and exceeded all testing requirements. Here is a quick map to verify our submission:

| Requirement | Your Target | Our Count | Location in README |
| :--- | :--- | :--- | :--- |
| **Correctness Tests** | 25 Tests | **80+ Cases** | [See Correctness Tests](#-correctness-tests-80-cases) |
| **Speedup Tests** | 10 Tests | **10 Cases** | [See Performance Tests](#-performance--speedup-tests-15-cases) |
| **Implementations** | 5 Variants | **5 Variants** | [See Implementations](#-5-parallel-implementations) |


## ðŸ› ï¸ Build & Installation

1.  **Prerequisites:**
    * OS: Linux / macOS
    * Compiler: GCC (with OpenMP support), MPI (OpenMPI / MPICH)
    * Python: 3.8+ (Required libraries: `numpy`, `pytest`, `mpi4py`, `matplotlib`)

2.  **Build the C Kernel:**
    The core simulation logic is written in C for maximum performance. You **must** compile it before running any Python script.

    ```bash
    cd csrc
    make clean && make
    cd ..
    ```
    *This generates `python/libpacman.so`, which is required for the Python wrapper.*

---

## ðŸ§ª Correctness Tests 

We utilize `pytest` to run a comprehensive suite of unit and integration tests. The test suite covers game mechanics, thread safety, memory isolation, and MPI consistency.

**How to Run All Correctness Tests:**
```bash
cd python/tests
# Run the full suite with verbose output
pytest -v verify_00_machanics.py level1/correctness/verify_01_thread_safety.py level2/correctness/verify_02_mem_isolation.py level3/correctness/verify_03_mpi_consistemcy.py
```
### Detailed Test Breakdown
**A. Game Mechanics & Scalability Stress (`verify_00_machanics.py`)**
- **Basic Integrity (5 Tests)**: Validates initialization, observation shapes, action bounds, reward structure, and reset mechanisms.
- **Physics Logic (2 Tests)**:Wall Collision: Verifies agents cannot walk through walls.Pacman Capture: Verifies game-over state and penalties when a ghost catches Pacman.
- **Scalability Stress Matrix (20 Tests)** :Uses `@pytest.mark.parametrize` to test 4 Grid Sizes (10x10 to 200x200) $\times$ 5 Agent Counts (1 to 4096). This ensures the C kernel handles extreme memory loads without segfaults.

**B. Level 1: Thread Safety (`level1/correctness/verify_01_thread_safety.py`)**
- **Thread Scaling Consistency (12 Tests)**: Compares simulation results across [2, 4, 8, 16] threads against a single-threaded baseline for 3 different step counts.
- **Agent Scaling (8 Tests)**: Verifies consistency for [16, 32, 64, 128] agents.
- **Edge Cases (6 Tests)**: Tests scenarios with "Half Dead" and "All Dead" ghosts to ensure logic holds when agents are inactive.
- **Deterministic Replay (4 Tests)**: Ensures different random seeds produce unique but reproducible results across thread counts.

**C. Level 2: Memory Isolation (`level2/correctness/verify_02_mem_isolation.py`)**
- **Environment Vectorization (20 Tests)**:Verifies that the parallel step_env_apply_actions_batch produces identical results to the sequential baseline across various n_envs (1 to 32) and thread counts.
- **Collision Isolation**: Ensures that walls and collisions in one environment do not bleed over into the memory space of another.

**D. Level 3: MPI Consistency (`level3/correctness/verify_03_mpi_consistemcy.py`)**
- **Consistency Check**: Verifies that running 16 environments on 1 MPI rank produces the exact same checksum as running them on 4 MPI ranks.
- **Load Balancing**: Tests correct handling of remainder environments (e.g., 10 envs on 3 ranks).
- **Oversubscription**: Verifies behavior when `rank_count > env_count`.
---
# ðŸš€ Performance & Speedup Tests 

We define 10 distinct performance scenarios to analyze speedup across different architectural levels. These are split into Experimental Analysis (`exp_`) and Full System Benchmarks (`run_g`).

### Part 1: Micro-Benchmarks & Scaling Analysis

Located in `python/tests/`

1.`exp_01_micro_scaling.sh:`
  - Goal: Analyzes Level 1 (Agent-parallelism) scaling.
  - Metric: Speedup vs. Agent Count (16 to 4096).

2.`exp_02_throughput.sh:`
- Goal: Analyzes Level 2 (Environment-parallelism) strong scaling.

- Result: 31.05x speedup on 32 threads (Linear Scaling).

3.`exp_03_mpi_scaling.sh:`

- Goal: Analyzes Level 3 (MPI) distributed weak scaling across nodes.

- Result: Scaled to 144 ranks with ~290k steps/s throughput.

4.`exp_04_hybrid_tradeoff.sh:`

- Goal: Compares Latency vs. Throughput between Hybrid and Pure MPI modes.

5.`exp_05_hybrid_multilevel.sh:`

- Goal: Evaluates hierarchical parallelism (MPI nodes + OpenMP threads).

### Part 2: Full System Scenarios

Located in `python/`


6.`run_g1_singlenode.sh:`

- Goal: Baseline performance measurement on a single compute node.

7.`run_g2_cluster_baseline.sh:`

- Goal: Multi-node baseline without specific optimizations.

8.`run_g3_hybrid_opt.sh:`

- Goal: optimized Hybrid execution (e.g., 4 MPI Ranks x 8 OpenMP Threads).

9.`run_g4_pure_mpi.sh:`

- Goal: Pure MPI execution (1 process per core). (Best Distributed Performance).

10.`run_g5_edge_detection.sh:`

- Goal: Measures communication overhead for "Ghost Exchange" (Halo regions) logic.

**How to Run a Performance Test:**
```bash
# Example: Run level 2 Speedup Test
cd python/tests
./exp_02_throughput.sh

# Exmaple: Run Pure MPI Benchmark
cd python
./run_g4_pure_mpi.sh
```
---
# ðŸ§  5 Parallel Implementations

To explore the trade-offs in parallel RL simulation, we implemented and compared 5 distinct variants:

1. **Sequential (Baseline)**: Single-threaded C kernel (`csrc/step_apply_sequential.c`). Used for correctness verification.

2. **Level 1 (Fine-Grained)**: OpenMP parallelism over Agents within a single environment (`python/tests/level1/speedup/worker_01_micro.py`). Best for few environments with complex agents.

3. **Level 2 (Coarse-Grained)**: OpenMP parallelism over Environments (`python/tests/level2/speedup/worker_02_throughput.py`). Best Single-Node Performance (31x speedup).

4. **Level 3 (Pure MPI)**: Distributed parallelism using MPI processes (`python/run_g4_pure_mpi.sh`). Best for scaling beyond one node.

5. **Hybrid (MPI + OpenMP)**: Hierarchical parallelism (`python/run_g3_hybrid_opt.sh`). Uses MPI across nodes and OpenMP within nodes.

---
# ðŸ“‚ Project Structure
```
.
â”œâ”€â”€ csrc/               # C Source Code (Kernel)
â”‚   â”œâ”€â”€ common.h        # Shared data structures
â”‚   â”œâ”€â”€ step_*.c        # Core game logic
â”‚   â””â”€â”€ Makefile        # Build script
â”œâ”€â”€ python/             # Python Wrapper & Logic
â”‚   â”œâ”€â”€ pacman_env.py   # ctypes interface class
â”‚   â”œâ”€â”€ run_g*.sh       # Full System Scenarios (G1-G5)
â”‚   â””â”€â”€ tests/          # Experiment scripts
â”‚       â”œâ”€â”€ exp_*.sh    # Analysis Experiments (01-05)
â”‚       â”œâ”€â”€ verify_*.py # Correctness Tests
â”‚       â”œâ”€â”€ level1/     # Thread parallelism
â”‚       â”œâ”€â”€ level2/     # Environment parallelism
â”‚       â”œâ”€â”€ level3/     # Distributed parallelism
â”‚       â””â”€â”€ results/    # Logs, plots from Microbenchmarks
â””â”€â”€ results/            # Logs from Full System Scenarios
```

## ðŸ“Š Results Summary

Full results and detailed analysis can be found in `final_performance_report.txt` and the formal Written Report.

### 1. Single-Node Performance (64 Cores)
We compared thread-based (OpenMP) vs. process-based (MPI) parallelism on a single node:

* **Pure MPI (Level 3):** **153,230 steps/s** (Recommended ðŸ†)
* **OpenMP (Level 2):** 4,742 steps/s
* **Observation:** Pure MPI outperforms OpenMP by **>30x** in this Python-wrapped environment, demonstrating that process-based isolation is significantly more efficient for this specific workload.

### 2. Distributed Scaling (Cluster)
* **Peak Throughput:** Scaled effectively to **144 MPI Ranks**, reaching **~290,000 steps/s**.
* **Scalability:** The system demonstrates strong scaling across multiple nodes using the implemented "Ghost Exchange" communication pattern.

### 3. Conclusion
* **Recommendation:** **Pure MPI (Level 3)** is the primary recommended architecture for high-performance simulation.